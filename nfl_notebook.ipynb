{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.split_dataframe import split_df\n",
    "from src.remove_correlated_stats import remove_corr_stats\n",
    "from src.my_predicted_stats import my_pred_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split df_train in half to help with visualizing all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1 = df_train.iloc[:,3:12]\n",
    "df_train_2 = df_train.iloc[:,12:20]\n",
    "df_train_3 = df_train.iloc[:,20:30]\n",
    "df_train_4 = df_train.iloc[:,30:40]\n",
    "df_train_5 = df_train.iloc[:,40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax = sns.heatmap(df_train_1.corr(), annot=True, cmap='coolwarm')\n",
    "ax.set_title('Correlation Map')\n",
    "plt.savefig('images/heat_map_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax = sns.heatmap(df_train_2.corr(), annot=True, cmap='coolwarm')\n",
    "ax.set_title('Correlation Map')\n",
    "plt.savefig('images/heat_map_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax = sns.heatmap(df_train_3.corr(), annot=True, cmap='coolwarm')\n",
    "ax.set_title('Correlation Map')\n",
    "plt.savefig('images/heat_map_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax = sns.heatmap(df_train_4.corr(), annot=True, cmap='coolwarm')\n",
    "ax.set_title('Correlation Map')\n",
    "plt.savefig('images/heat_map_4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax = sns.heatmap(df_train_5.corr(), annot=True, cmap='coolwarm')\n",
    "ax.set_title('Correlation Map')\n",
    "plt.savefig('images/heat_map_5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based off of the EDA, decided to drop the following columns:\n",
    "columns_to_remove = ['passAttempts', 'passYardPerAtt', 'passIntPct', 'pass40Plus', 'sacks_allowed_yard','rush1stdowns', 'rush40plus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier - My Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/aggregated_2014_to_2019.csv')\n",
    "df = df.sort_values(by=['season','week','game_id']).reset_index(drop=True)\n",
    "df = remove_corr_stats(df)\n",
    "my_df = my_pred_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = my_df.sort_values(by=['season','week','game_id']).reset_index(drop=True)\n",
    "df_train, df_test = split_df(my_df, 0.3)\n",
    "y_train = np.array(df_train.pop('win_game'))\n",
    "X_train = np.array(df_train.iloc[:,5:])\n",
    "y_test = np.array(df_test.pop('win_game'))\n",
    "X_test = np.array(df_test.iloc[:,5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 fold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5969230769230769\n",
      "Precision: 0.5963369506786318\n",
      "Recall: 0.5890426660354862\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10)\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "for train_index, test_index in kfold.split(X_train):\n",
    "    model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "    model.fit(X_train[train_index], y_train[train_index])\n",
    "    y_predict = model.predict(X_train[test_index])\n",
    "    y_true = y_train[test_index]\n",
    "    accuracies.append(accuracy_score(y_true, y_predict))\n",
    "    precisions.append(precision_score(y_true, y_predict))\n",
    "    recalls.append(recall_score(y_true, y_predict))\n",
    "print(\"Accuracy:\", np.average(accuracies))\n",
    "print(\"Precision:\", np.average(precisions))\n",
    "print(\"Recall:\", np.average(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "df_test['win_perc'] = model.predict_proba(X_test)[:,1]\n",
    "df_test['win_game'] = y_test\n",
    "summarized_df = df_test[['game_id','season', 'week','team','opponent','win_perc','win_game']]\n",
    "summarized_df = summarized_df.sort_values(by='game_id').reset_index(drop=True)\n",
    "summarized_df.reset_index(drop=True, inplace=True)\n",
    "summarized_df['predicted_win'] = None\n",
    "for i in range(0, len(summarized_df), 2):\n",
    "    if summarized_df.loc[i, 'win_perc'] > summarized_df.loc[i+1, 'win_perc']:\n",
    "        summarized_df.loc[i, 'predicted_win'] = 1\n",
    "        summarized_df.loc[i+1, 'predicted_win'] = 0\n",
    "    else:\n",
    "        summarized_df.loc[i, 'predicted_win'] = 0\n",
    "        summarized_df.loc[i+1, 'predicted_win'] = 1\n",
    "y_true = np.array(summarized_df['win_game'])\n",
    "y_predict = np.array(summarized_df['predicted_win'])\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_predict))\n",
    "print(\"Precision:\", precision_score(y_true, y_predict))\n",
    "print(\"Recall:\", recall_score(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                   index = df_train.columns[5:],\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier - All Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/aggregated_2014_to_2019.csv')\n",
    "df = df.sort_values(by=['season','week','game_id']).reset_index(drop=True)\n",
    "df = remove_corr_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['season','week','game_id']).reset_index(drop=True)\n",
    "df_train, df_test = split_df(df, 0.3)\n",
    "y_train = np.array(df_train.pop('win_game'))\n",
    "X_train = np.array(df_train.iloc[:,5:])\n",
    "y_test = np.array(df_test.pop('win_game'))\n",
    "X_test = np.array(df_test.iloc[:,5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 fold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10)\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "for train_index, test_index in kfold.split(X_train):\n",
    "    model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "    model.fit(X_train[train_index], y_train[train_index])\n",
    "    y_predict = model.predict(X_train[test_index])\n",
    "    y_true = y_train[test_index]\n",
    "    accuracies.append(accuracy_score(y_true, y_predict))\n",
    "    precisions.append(precision_score(y_true, y_predict))\n",
    "    recalls.append(recall_score(y_true, y_predict))\n",
    "print(\"Accuracy:\", np.average(accuracies))\n",
    "print(\"Precision:\", np.average(precisions))\n",
    "print(\"Recall:\", np.average(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "df_test['win_perc'] = model.predict_proba(X_test)[:,1]\n",
    "df_test['win_game'] = y_test\n",
    "summarized_df = df_test[['game_id','season', 'week','team','opponent','win_perc','win_game']]\n",
    "summarized_df = summarized_df.sort_values(by='game_id').reset_index(drop=True)\n",
    "summarized_df.reset_index(drop=True, inplace=True)\n",
    "summarized_df['predicted_win'] = None\n",
    "for i in range(0, len(summarized_df), 2):\n",
    "    if summarized_df.loc[i, 'win_perc'] > summarized_df.loc[i+1, 'win_perc']:\n",
    "        summarized_df.loc[i, 'predicted_win'] = 1\n",
    "        summarized_df.loc[i+1, 'predicted_win'] = 0\n",
    "    else:\n",
    "        summarized_df.loc[i, 'predicted_win'] = 0\n",
    "        summarized_df.loc[i+1, 'predicted_win'] = 1\n",
    "y_true = np.array(summarized_df['win_game'])\n",
    "y_predict = np.array(summarized_df['predicted_win'])\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_predict))\n",
    "print(\"Precision:\", precision_score(y_true, y_predict))\n",
    "print(\"Recall:\", recall_score(y_true, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                   index = df_train.columns[5:],\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
